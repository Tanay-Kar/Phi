{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments page for the programming Language 'Phi'\n",
    "It is a project aiming to create a math friendly programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the line which will be used throughout the notebook\n",
    "TEST_LINE = \"10 + b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    \"\"\"Base class for exceptions in this module.\"\"\"\n",
    "    def __init__(self, msg):\n",
    "        self.msg = msg\n",
    "        super().__init__(self.msg)\n",
    "\n",
    "\n",
    "class ParseError(Error):\n",
    "    \"\"\"Exception raised for errors in the input.\n",
    "    Attributes:\n",
    "        msg  -- explanation of the error\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, msg):\n",
    "        self.msg = 'PARSING ERROR : ' + msg\n",
    "        super().__init__(self.msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexer\n",
    "r_ID = r'[a-zA-Z_][a-zA-Z0-9_]*'\n",
    "r_Num = r'(\\d+)\\.?(\\d+)*'\n",
    "r_Assign = r'='\n",
    "r_Comma = r'\\,'\n",
    "r_Plus = r'\\+'\n",
    "r_Minus = r'-'\n",
    "r_Mult = r'\\*'\n",
    "r_Div = r'/'\n",
    "r_Dot = r'\\.'\n",
    "r_Caret = r'\\^'\n",
    "r_lParen = r'\\('\n",
    "r_rParen = r'\\)'\n",
    "r_lBrace = r'\\{'\n",
    "r_rBrace = r'\\}'\n",
    "\n",
    "# Keywords\n",
    "keywords = {\n",
    "    'func': 'FUNC',\n",
    "    'print': 'PRINT',\n",
    "    'show': 'SHWTBL',\n",
    "    'return': 'RETURN',\n",
    "    'plot': 'PLT',\n",
    "}\n",
    "\n",
    "# Token Class\n",
    "\n",
    "\n",
    "class Token:\n",
    "    def __init__(self, type, value=None):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"< Token {self.type} :: '{self.value}' >\" if self.value else f'< Token {self.type} >'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class TupleToken(Token):\n",
    "    def __init__(self):\n",
    "        super().__init__(type='TUPLE')\n",
    "        self.variables = []\n",
    "        self.values = []\n",
    "\n",
    "    def add(self, token):\n",
    "        if isinstance(token, Token):\n",
    "            self.variables.append(str(token))\n",
    "            self.values = self.variables\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"< Tuple :: {' ,'.join(self.variables)} >\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexer Class\n",
    "\n",
    "class Lexer:\n",
    "    def __init__(self, line) -> None:\n",
    "        self.line = line\n",
    "        self.pos = -1\n",
    "        self.char = None\n",
    "        self.token_corpus = []\n",
    "        self.advance()\n",
    "\n",
    "    def advance(self):\n",
    "        if self.pos < len(self.line) - 1:\n",
    "            self.pos += 1\n",
    "            self.char = self.line[self.pos]\n",
    "\n",
    "        else:\n",
    "            self.char = None\n",
    "            self.token_corpus.append(Token('EOL'))\n",
    "\n",
    "    def peek(self):\n",
    "        return self.line[self.pos + 1] if self.pos < len(self.line) - 1 else 'EOL'\n",
    "\n",
    "    def assert_char(self, char, tokentype):\n",
    "        if re.fullmatch(tokentype, char):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def make_number(self):\n",
    "        num = self.char\n",
    "        next_char = self.peek()\n",
    "        while self.assert_char(num+next_char, r_Num) and next_char != 'EOL':\n",
    "            num += next_char\n",
    "            self.advance()\n",
    "            next_char = self.peek()\n",
    "\n",
    "        return Token('NUMBER', num)\n",
    "\n",
    "    def make_id(self):\n",
    "        if not self.assert_char(self.char, r_ID):\n",
    "            return\n",
    "        id = self.char\n",
    "        next_char = self.peek()\n",
    "        while self.assert_char(id+next_char, r_ID) and next_char != 'EOL':\n",
    "            id += next_char\n",
    "            self.advance()\n",
    "            next_char = self.peek()\n",
    "\n",
    "        if id in keywords:\n",
    "            return Token(keywords[id])\n",
    "        else:\n",
    "            return Token('ID', id)\n",
    "\n",
    "    def get_tokens(self):\n",
    "        while self.char != None:\n",
    "            if self.assert_char(self.char, r_Num):\n",
    "                self.token_corpus.append(self.make_number())\n",
    "            elif self.assert_char(self.char, r_ID):\n",
    "                self.token_corpus.append(self.make_id())\n",
    "            elif self.assert_char(self.char, r_lParen):\n",
    "                self.token_corpus.append(Token('LPAREN'))\n",
    "            elif self.assert_char(self.char, r_rParen):\n",
    "                self.token_corpus.append(Token('RPAREN'))\n",
    "            elif self.assert_char(self.char, r_Comma):\n",
    "                self.token_corpus.append(Token('COMMA'))\n",
    "            elif self.assert_char(self.char, r_Plus):\n",
    "                self.token_corpus.append(Token('PLUS'))\n",
    "            elif self.assert_char(self.char, r_Minus):\n",
    "                self.token_corpus.append(Token('MINUS'))\n",
    "            elif self.assert_char(self.char, r_Mult):\n",
    "                self.token_corpus.append(Token('MULT'))\n",
    "            elif self.assert_char(self.char, r_Div):\n",
    "                self.token_corpus.append(Token('DIV'))\n",
    "            elif self.assert_char(self.char, r_Dot):\n",
    "                self.token_corpus.append(Token('DOT'))\n",
    "            elif self.assert_char(self.char, r_Assign):\n",
    "                self.token_corpus.append(Token('ASSIGN'))\n",
    "            elif self.assert_char(self.char, r_Caret):\n",
    "                self.token_corpus.append(Token('CARET'))\n",
    "            elif self.assert_char(self.char, r_lBrace):\n",
    "                self.token_corpus.append(Token('LBRACE'))\n",
    "            elif self.assert_char(self.char, r_rBrace):\n",
    "                self.token_corpus.append(Token('RBRACE'))\n",
    "\n",
    "            elif self.char == ' ':\n",
    "                pass\n",
    "            elif self.char == '\\n':\n",
    "                self.token_corpus.append(Token('EOL'))\n",
    "            else:\n",
    "                raise Exception(f'Invalid Character: {self.char}')\n",
    "            self.advance()\n",
    "        return self.token_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[< Token NUMBER :: '10' >, < Token PLUS >, < Token ID :: 'b' >, < Token EOL >]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "l = Lexer(TEST_LINE)\n",
    "l.get_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser - Expreiment 01 : Expression Parser\n",
    "Experiments for parsing an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[< Token NUMBER :: '10' >, < Token PLUS >, < Token ID :: 'b' >, < Token EOL >]\n"
     ]
    }
   ],
   "source": [
    "tokens = l.get_tokens()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinOpNode:\n",
    "    def __init__(self, left, operator, right):\n",
    "        self.type = \"BINOP\"\n",
    "        self.left = left\n",
    "        self.operator = operator\n",
    "        self.right = right\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"({self.left} {self.operator} {self.right})\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class FactorNode:\n",
    "    def __init__(self, value, sign='+'):\n",
    "        self.type = \"FACTOR\"\n",
    "        self.value = value if sign == '+' else '-' + value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.value.value}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class ExpressionNode:\n",
    "    def __init__(self, expression, type_hint=None):\n",
    "        self.type = \"EXPRESSION\"\n",
    "        self.expression = expression\n",
    "        self.type_hint = type_hint # Used for a special case when the expression is a single number\n",
    "        self.value = f\"Expression {self.expression}\"\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return f\"< Expression {self.expression}>\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class DeclarationNode:\n",
    "    def __init__(self, func: Token, args: TupleToken):\n",
    "        self.type = \"FUNCTION\"\n",
    "        self.function_name = func.value\n",
    "        self.args = args\n",
    "        self.value = f\"Function {self.function_name} args {self.args.variables}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"< Function {self.function_name} args {self.args.variables}>\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionParser:\n",
    "    def __init__(self, tokens):\n",
    "        self.master_tokens = tokens\n",
    "        self.tokens = tokens\n",
    "        self.current_token: Token = None\n",
    "        self.index = -1\n",
    "        self.startindex, self.endindex = 0, 0\n",
    "        self.advance()\n",
    "\n",
    "    def advance(self):\n",
    "        while self.index < len(self.tokens):\n",
    "            self.current_token = self.tokens[self.index]\n",
    "            self.index += 1\n",
    "            if self.current_token.type != 'EOL':\n",
    "                break\n",
    "\n",
    "    def peek(self):\n",
    "        return self.tokens[self.index+1] if self.index+1 < len(self.tokens) else None\n",
    "\n",
    "    def return_slice(self):\n",
    "        return self.startindex, self.endindex\n",
    "\n",
    "    def parse(self):\n",
    "        self.startindex = self.index - 1\n",
    "        parsed = self.parse_expression()\n",
    "        self.endindex = self.index - 1\n",
    "\n",
    "        if getattr(parsed, 'type', None) is not None and parsed.type == \"FACTOR\":\n",
    "            if parsed.value.type in ('ID', 'FUNCTION'):\n",
    "                parsed = parsed.value\n",
    "                return parsed\n",
    "            if parsed.value.type == 'NUMBER':\n",
    "                parsed = parsed.value.value\n",
    "                return ExpressionNode(parsed, type_hint='NUM')\n",
    "            \n",
    "        elif not parsed:\n",
    "            return parsed\n",
    "        return ExpressionNode(parsed)\n",
    "\n",
    "    def parse_expression(self):\n",
    "        left = self.parse_term()\n",
    "        while self.current_token.type in ('PLUS', 'MINUS'):\n",
    "            op = self.current_token.type\n",
    "            self.advance()\n",
    "            right = self.parse_term()\n",
    "            left = BinOpNode(left, op, right)\n",
    "        return left\n",
    "\n",
    "    def parse_term(self):\n",
    "        left = self.parse_radical()\n",
    "        while self.current_token.type in ('MULT', 'DIV', 'DOT'):\n",
    "            op = self.current_token.type if self.current_token.type not in (\n",
    "                'DOT') else 'MULT'\n",
    "            self.advance()\n",
    "            right = self.parse_radical()\n",
    "            left = BinOpNode(left, op, right)\n",
    "\n",
    "        return left\n",
    "\n",
    "    def parse_radical(self):\n",
    "        left = self.parse_factor()\n",
    "        while self.current_token.type == 'CARET':\n",
    "            op = self.current_token.type\n",
    "            self.advance()\n",
    "            right = self.parse_factor()\n",
    "            left = BinOpNode(left, op, right)\n",
    "\n",
    "        return left\n",
    "\n",
    "    def parse_factor(self):\n",
    "        if self.current_token.type == 'NUMBER':\n",
    "\n",
    "            value = self.current_token\n",
    "            self.advance()\n",
    "            return FactorNode(value)\n",
    "\n",
    "        elif self.current_token.type == 'ID':\n",
    "            value = self.current_token\n",
    "            self.advance()\n",
    "            return FactorNode(value)\n",
    "\n",
    "        elif self.current_token.type == 'FUNCTION':\n",
    "            value = self.current_token\n",
    "            self.advance()\n",
    "            return FactorNode(value)\n",
    "\n",
    "        elif self.current_token.type == 'MINUS':  # Unary negation handling\n",
    "            self.advance()\n",
    "            right = self.parse_factor()\n",
    "            return FactorNode(right, sign='-')\n",
    "\n",
    "        # Handle opening parenthesis \"(\"\n",
    "        elif self.current_token.type == 'LPAREN':\n",
    "            self.advance()\n",
    "            expr = self.parse_expression()\n",
    "            if self.current_token.type == 'COMMA':\n",
    "                pass  # Ensure closing parenthesis \")\"\n",
    "            elif self.current_token.type == 'RPAREN':  # Ensure closing parenthesis \")\"\n",
    "                self.advance()\n",
    "                return expr\n",
    "            else:\n",
    "                raise SyntaxError(\"Missing closing parenthesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionParserWrapper:\n",
    "    def __init__(self, tokens):\n",
    "        self.index = -1\n",
    "        self.tokens = tokens\n",
    "        self.current_token: Token = None\n",
    "\n",
    "        self.advance()\n",
    "\n",
    "    def advance(self):\n",
    "        while self.index < len(self.tokens):\n",
    "            self.current_token = self.tokens[self.index]\n",
    "            self.index += 1\n",
    "            if self.current_token.type != \"EOL\":\n",
    "                break\n",
    "\n",
    "    def return_tokens(self):\n",
    "        return self.tokens\n",
    "\n",
    "    def parse(self):\n",
    "        while self.current_token.type != \"EOL\":\n",
    "            print(self.current_token)\n",
    "            ep = ExpressionParser(self.tokens[self.index-1:])\n",
    "            pr = ep.parse()\n",
    "            sl1, sl0 = [i + self.index - 1 for i in ep.return_slice()]\n",
    "            if pr:\n",
    "                self.tokens[sl1:sl0] = [pr]\n",
    "                self.index = sl1 + 1\n",
    "            self.advance()\n",
    "        return self.return_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[< Token NUMBER :: '10' >, < Token PLUS >, < Token ID :: 'b' >, < Token EOL >]\n",
      "< Expression (10 PLUS b)>\n",
      "(0, 3)\n",
      "[< Token NUMBER :: '10' >, < Token PLUS >, < Token ID :: 'b' >]\n"
     ]
    }
   ],
   "source": [
    "p = ExpressionParser(tokens)\n",
    "print(tokens)\n",
    "print(p.parse())\n",
    "sl = p.return_slice()\n",
    "print(sl)\n",
    "print(tokens[sl[0]:sl[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser - Experiment 02 : Tuple Parser\n",
    "Experiments for parsing a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TupleParser:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.current_token = tokens[0]\n",
    "        self.tuple = TupleToken\n",
    "        self.index = -1\n",
    "        self.item_corpus = []\n",
    "        self.advance()\n",
    "\n",
    "    def advance(self):\n",
    "        self.index += 1\n",
    "        if self.index < len(self.tokens):\n",
    "            self.current_token = self.tokens[self.index]\n",
    "        else:\n",
    "            self.current_token = Token('EOL')\n",
    "\n",
    "    def couplet(self, items, stindex):\n",
    "        return items[stindex], items[stindex+1]\n",
    "\n",
    "    def assert_type(self, token, type, enforceable=False, alt_type=None):\n",
    "        if token.type not in (type, alt_type):\n",
    "            if enforceable:\n",
    "                raise Exception('Expected token type ' +\n",
    "                                type + ' but got ' + token.type)\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def parse_item(self, items):\n",
    "        tupletok = TupleToken()\n",
    "        index = 0\n",
    "        # check if the list is empty\n",
    "        if len(items) == 0:\n",
    "            return tupletok\n",
    "        # check if the list has only one item\n",
    "        elif len(items) == 1:\n",
    "            if self.assert_type(items[0], 'ID') or self.assert_type(items[0], 'NUMBER'):\n",
    "                tupletok.add(items[0])\n",
    "                return tupletok\n",
    "\n",
    "        # check if the list has the format of a tuple\n",
    "        id, sep = self.couplet(items, 0)\n",
    "        if (self.assert_type(id, 'ID') or self.assert_type(id, 'NUMBER')) and self.assert_type(sep, 'COMMA'):\n",
    "            tupletok.add(id)\n",
    "            index += 1\n",
    "            while index < len(items):\n",
    "                sep, id = self.couplet(items, index)\n",
    "                self.assert_type(sep, 'COMMA', enforceable=True)\n",
    "                self.assert_type(id, 'ID', enforceable=True, alt_type='NUMBER')\n",
    "                tupletok.add(id)\n",
    "                index += 2\n",
    "\n",
    "            return tupletok\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def replace(self, st, end, new_item):\n",
    "        self.tokens[st:end] = [new_item]\n",
    "        self.index = st\n",
    "\n",
    "    def parse(self):\n",
    "        t_items = []\n",
    "        while self.index < len(self.tokens):\n",
    "            if self.current_token.type == 'LPAREN':\n",
    "                st = self.index\n",
    "                while self.current_token.type != 'RPAREN' and self.current_token.type != 'EOL':\n",
    "                    t_items.append(self.current_token)\n",
    "                    self.advance()\n",
    "                if self.tokens[self.index].type == 'RPAREN':\n",
    "                    t_items.pop(0)\n",
    "                    end = self.index + 1\n",
    "                    self.replace(st, end, self.parse_item(t_items))\n",
    "\n",
    "                    t_items = []\n",
    "\n",
    "                elif self.tokens[self.index].type == 'EOL':\n",
    "                    print('No closing parenthesis')\n",
    "\n",
    "            self.advance()\n",
    "\n",
    "        self.index = -1\n",
    "        self.parse_declarations()\n",
    "        return self.tokens\n",
    "\n",
    "    def parse_declarations(self):\n",
    "        while self.index < len(self.tokens):\n",
    "            if self.current_token and self.current_token.type == 'ID' and self.index + 1 < len(self.tokens) and self.tokens[self.index+1].type == 'TUPLE':\n",
    "                print('Declaration')\n",
    "                self.replace(self.index, self.index+2,\n",
    "                             DeclarationNode(self.current_token, self.tokens[self.index+1]))\n",
    "\n",
    "            self.advance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declaration\n",
      "< Token PLT >\n",
      "< Function f args [\"< Token ID :: 'x' >\"]>\n",
      "[< Token PLT >, < Function f args [\"< Token ID :: 'x' >\"]>, < Token EOL >]\n"
     ]
    }
   ],
   "source": [
    "l = \"plot f(x)\"\n",
    "le = Lexer(l)\n",
    "l = le.get_tokens()\n",
    "# print(l)\n",
    "tp = TupleParser(l)\n",
    "l = tp.parse()\n",
    "# print(l)\n",
    "epw = ExpressionParserWrapper(l)\n",
    "tok = epw.parse()\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineNode:\n",
    "    def __init__(self, tokens:list, specid:str, primarykeyword:str, grammar:dict):\n",
    "        self.type = \"LINE\"\n",
    "        self.tokens = tokens\n",
    "        self.grammar = grammar\n",
    "        self.mastergrammar = specid\n",
    "        self.primarykeyword = primarykeyword\n",
    "        self.function = self.grammar[self.primarykeyword]['function']        \n",
    "        \n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"[LINE grammar {self.mastergrammar[0]}:{self.mastergrammar[0:]} <Contents{self.tokens}>]\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MasterParser:\n",
    "    def __init__(self, tokens, grammar):\n",
    "        self.tokens = tokens\n",
    "        self.grammar_raw = grammar\n",
    "        self.master_lexeme = None\n",
    "        self.applicable_rules = []\n",
    "        self.absolute_rule = None\n",
    "        self.index = -1\n",
    "        self.current_token: Token = None\n",
    "        self.enforce_grammar = False\n",
    "        self.advance()\n",
    "\n",
    "    def reset_index(self):\n",
    "        self.index = -1\n",
    "        self.current_token: Token = None\n",
    "        self.advance()\n",
    "\n",
    "    def advance(self):\n",
    "        while self.index < len(self.tokens):\n",
    "            self.current_token = self.tokens[self.index]\n",
    "            self.index += 1\n",
    "            if self.current_token.type != \"EOL\":\n",
    "                break\n",
    "\n",
    "    def get_master_rule(self):\n",
    "        first_token_type = self.current_token.type\n",
    "        self.primary_keyword = first_token_type\n",
    "        if first_token_type in self.grammar_raw:\n",
    "            self.master_lexeme = first_token_type\n",
    "            self.applicable_rules = [self.grammar_raw[first_token_type]['syntax'][i]['syntax'].split()\n",
    "                                     for i in self.grammar_raw[first_token_type]['syntax']\n",
    "                                     ]\n",
    "            self.rule_ids = [self.grammar_raw[first_token_type]['syntax'][i]['spec_id']\n",
    "                              for i in self.grammar_raw[first_token_type]['syntax']\n",
    "                              ]\n",
    "\n",
    "    def get_item(self, index, list):\n",
    "        if index < len(list):\n",
    "            return list[index]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def grammer_match(self):\n",
    "        print(self.applicable_rules)\n",
    "        for i in range(len(self.tokens)):\n",
    "            self.enforce_grammar = True if len(\n",
    "                self.applicable_rules) == 1 else False\n",
    "            rules,rules_id = [],[]\n",
    "            \n",
    "            if self.enforce_grammar:\n",
    "                if self.get_item(i, self.applicable_rules[0]) != None:\n",
    "                    if self.get_item(i, self.applicable_rules[0]) == self.tokens[i].type or self.get_item(i, self.applicable_rules[0]) == getattr(self.tokens[i], 'type_hint', None):\n",
    "                        pass\n",
    "                    else:\n",
    "                        if self.get_item(i, self.applicable_rules[0]) == 'EOL':\n",
    "                            raise ParseError(f'Unexpected {self.tokens[i].type} \\'{self.tokens[i].value}\\'')\n",
    "                        raise ParseError(f'Expected {self.get_item(i,self.applicable_rules[0])} but recieved {self.tokens[i].type} \\'{self.tokens[i].value}\\'')\n",
    "                else:\n",
    "                    raise ParseError(f'Unexpected {self.tokens[i].type} \\'{self.tokens[i].value}\\'')\n",
    "\n",
    "            else:\n",
    "                for j in range(len(self.applicable_rules)):\n",
    "                    rule = self.applicable_rules[j]\n",
    "                    id = self.rule_ids[j]\n",
    "                    if self.get_item(i, rule) == self.tokens[i].type or self.get_item(i, rule) == getattr(self.tokens[i], 'type_hint', None):\n",
    "                        rules.append(rule)\n",
    "                        rules_id.append(id)\n",
    "                        print(self.tokens[i].type, 'matched', rule)\n",
    "                        print(self.applicable_rules)\n",
    "                self.applicable_rules = rules\n",
    "                self.rule_ids = rules_id\n",
    "                \n",
    "        if len(self.applicable_rules) != 1:\n",
    "            raise ParseError(f'Syntax Error')\n",
    "        elif len(self.applicable_rules) == 1:\n",
    "            self.absolute_rule = self.applicable_rules[0]\n",
    "            self.absolute_rule_id = self.rule_ids[0]\n",
    "            print('Absolute Rule', self.absolute_rule)\n",
    "            print('Absolute Rule ID', self.absolute_rule_id)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def parse(self):\n",
    "        self.get_master_rule()\n",
    "        self.grammer_match()\n",
    "        line = LineNode(self.tokens,self.absolute_rule_id,self.primary_keyword,self.grammar_raw)\n",
    "        return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PLT', 'FUNCTION', 'EOL']]\n",
      "Absolute Rule ['PLT', 'FUNCTION', 'EOL']\n",
      "Absolute Rule ID G01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LINE grammar G:G01 <Contents[< Token PLT >, < Function f args [\"< Token ID :: 'x' >\"]>, < Token EOL >]>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('grammar.json', 'r') as f:\n",
    "    grammar = json.load(f)\n",
    "\n",
    "parser = MasterParser(tok, grammar)\n",
    "parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot\n"
     ]
    }
   ],
   "source": [
    "print(grammar['PLT']['function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
